# -*- coding: utf-8 -*-
"""Capstone_Deterministic_rule.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lRfvee285LQzThRy3BzZ0ctL02UTtzvN

# AIML Online Capstone - AUTOMATIC TICKET ASSIGNMENT

Pre-Processing, Data Visualization and EDA
● Exploring the given Data files
● Understanding the structure of data
● Missing points in data
● Finding inconsistencies in the data
● Visualizing different patterns
● Visualizing different text features
● Dealing with missing values
● Text preprocessing
● Creating word vocabulary from the corpus of report text data
● Creating tokens as required
"""

# Initialize the random number generator
import random
random.seed(0)

# Ignore the warnings
import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive/')

#Set your project path 
import os
project_path =os.chdir('/content/drive/My Drive/Colab Notebooks/Capstone')

import numpy as np
import pandas as pd
import tensorflow as tf
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS
#from tensorflow.contrib.rnn import BasicLSTMCell
from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn
import time
import os
from sklearn.model_selection import train_test_split

df = pd.read_excel (r'input_data.xlsx')

df.head()

df.describe()

df.isnull().sum()

nan_rows1 = df[df['Short description'].isnull()]
nan_rows2 = df[df['Description'].isnull()]

nan_rows1

nan_rows2

df.loc[df['Description'].isnull(),'Description'] = df['Short description']

df.loc[[4395]]

df.loc[df['Short description'].isnull(),'Short description'] = df['Description']

df.loc[[3924]]

df.isnull().sum()

"""Preprocess text"""

# Merge 'Short description' and 'Description' columns into single column 'description'
df['description'] = (df['Short description']+df['Description']).map(str)

df_Tickets = df.drop(['Description'], axis=1)

df_Tickets.head()

df_Tickets = df_Tickets.drop(['Short description'], axis=1)

df_Tickets.info()

df_Tickets['Assignment group'].nunique()

df_Tickets.head()

df_Tickets['description']= df_Tickets['description'].astype(str).str.replace('--@--',' ')

df_Tickets.head()

df_Tickets = df_Tickets.drop(['Caller'], axis=1)
df_Tickets.head()

df_Tickets['description'] = df_Tickets['description'].str.strip('[]')

df_Tickets['description'] = df_Tickets['description'].str.replace("'", " ")

df_Tickets.head()

df_Tickets.index = df_Tickets.index.astype(str)

df_Tickets

df_Tickets['Descrip_less'] = df_Tickets['description'].apply(lambda x : x.rsplit(maxsplit=len(x.split())-10)[0])

df_Tickets1 = df_Tickets.drop(['description'], axis=1)
df_Tickets1.head()

df_new = df_Tickets1.values.tolist()
#df_Tickets1 = df_Tickets.to_dict('range')
df_new

import json
class mydict(dict):
  def __str__(self):
    return json.dumps(self)

df_final =  json.dumps(df_new)
df_final

#pip install spacy

def callback_method (matcher, doc, i, matches):
  string_id = nlp.vocab.strings[match_id] 
  match_id, start, end in matches[i]
  span = doc[start:end]  # The matched span
  print(match_id, string_id, start, end, span.text)

import spacy
from collections import Counter
from spacy.matcher import PhraseMatcher
nlp = spacy.load("en_core_web_sm")
matcher = PhraseMatcher(nlp.vocab)
terms = ["GRP_8", "GRP_135", "what type of outage"]
# Only run nlp.make_doc to speed things up
patterns = [nlp.make_doc(text) for text in terms]
matcher.add("what type of outage", None, *patterns)
doc = nlp(df_final)
matches = matcher(doc)
d = []
for match_id, start, end in matches:
    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID
    span = doc[start : end]  # get the matched slice of the doc
    d.append((rule_id, span.text))

print("\n".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items()))

import spacy
from collections import Counter
from spacy.matcher import PhraseMatcher
nlp = spacy.load("en_core_web_sm")
matcher = PhraseMatcher(nlp.vocab)
terms = ["GRP_0", "GRP_24", "GRP_25", "EU_tool"]
# Only run nlp.make_doc to speed things up
patterns = [nlp.make_doc(text) for text in terms]
matcher.add("EU_tool", None, *patterns)
doc = nlp(df_final)
matches = matcher(doc)
d = []
for match_id, start, end in matches:
    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID
    span = doc[start : end]  # get the matched slice of the doc
    d.append((rule_id, span.text))
print("\n".join(f'{i} ({j})' for i,j in Counter(d).items()))

import spacy
from collections import Counter
from spacy.matcher import PhraseMatcher
nlp = spacy.load("en_core_web_sm")
matcher = PhraseMatcher(nlp.vocab)
terms = ["GRP_17", "reset password"]
# Only run nlp.make_doc to speed things up
patterns = [nlp.make_doc(text) for text in terms]
matcher.add("reset password", None, *patterns)
matcher.add("GRP_17", None, *patterns)
doc = nlp(df_final)
matches = matcher(doc)
d = []
for match_id, start, end in matches:
    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID
    span = doc[start : end]  # get the matched slice of the doc
    d.append((rule_id, span.text))
print("\n".join(f'{i} ({j})' for i,j in Counter(d).items()))

import spacy
from collections import Counter
from spacy.matcher import PhraseMatcher
nlp = spacy.load("en_core_web_sm")
matcher = PhraseMatcher(nlp.vocab)
terms = ["GRP_35", "need access"]
# Only run nlp.make_doc to speed things up
patterns = [nlp.make_doc(text) for text in terms]
matcher.add("need access", None, *patterns)
matcher.add("GRP_35", None, *patterns)
doc = nlp(df_final)
matches = matcher(doc)
d = []
for match_id, start, end in matches:
    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID
    span = doc[start : end]  # get the matched slice of the doc
    d.append((rule_id, span.text))
print("\n".join(f'{i} ({j})' for i,j in Counter(d).items()))

import spacy
from collections import Counter
from spacy.matcher import PhraseMatcher
nlp = spacy.load("en_core_web_sm")
matcher = PhraseMatcher(nlp.vocab)
terms = ["GRP_0", "windows account"]
# Only run nlp.make_doc to speed things up
patterns = [nlp.make_doc(text) for text in terms]
matcher.add("windows account", None, *patterns)
matcher.add("GRP_0", None, *patterns)
doc = nlp(df_final)
matches = matcher(doc)
d = []
for match_id, start, end in matches:
    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID
    span = doc[start : end]  # get the matched slice of the doc
    d.append((rule_id, span.text))
print("\n".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items()))

doc_new=(nlp.make_doc(text) for text in df_Tickets1)

df_Tickets['word_count'] = df_Tickets['description'].apply(lambda x: len(str(x).split(" ")))
df_Tickets.head()

